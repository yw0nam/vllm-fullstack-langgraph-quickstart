"""
Response processing module
AI ì‘ë‹µ ì²˜ë¦¬ ë° ìŠ¤íŠ¸ë¦¬ë° ê´€ë ¨ ê¸°ëŠ¥ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

import re
import streamlit as st
import traceback
from langchain_core.messages import HumanMessage


class ResponseProcessor:
    """ì‘ë‹µ ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, graph, sidebar_manager):
        self.graph = graph
        self.sidebar_manager = sidebar_manager

    def separate_thinking_and_answer(self, final_answer):
        """<think> íƒœê·¸ë¥¼ ë¶„ë¦¬í•˜ì—¬ reasoningê³¼ main answerë¡œ ë‚˜ëˆ•ë‹ˆë‹¤."""
        reasoning_text = ""
        main_answer = final_answer

        if final_answer:
            # <think>...</think> íŒ¨í„´ ì°¾ê¸°
            think_pattern = r"<think>(.*?)</think>"
            think_matches = re.findall(think_pattern, final_answer, re.DOTALL)

            if think_matches:
                reasoning_text = "\n".join(think_matches)
                # <think> íƒœê·¸ ë¶€ë¶„ì„ ì œê±°í•œ ë©”ì¸ ë‹µë³€
                main_answer = re.sub(
                    think_pattern, "", final_answer, flags=re.DOTALL
                ).strip()

        return main_answer, reasoning_text

    def render_final_result(self, main_answer, reasoning_text, collected_data):
        """ìµœì¢… ê²°ê³¼ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
        # ìµœì¢… ë‹µë³€ì„ ë¨¼ì € í‘œì‹œ (ìƒë‹¨)
        st.markdown("### ğŸ“‹ ìµœì¢… ë¦¬ì„œì¹˜ ê²°ê³¼")
        if main_answer:
            # ì¸ìš© ë§í¬ë¥¼ ì‹¤ì œ URLë¡œ ë³€í™˜
            enhanced_answer = self._enhance_citations(main_answer, collected_data)
            st.markdown(enhanced_answer)
        else:
            st.markdown("ë¦¬ì„œì¹˜ë¥¼ ì™„ë£Œí–ˆì§€ë§Œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # Reasoning ë¶€ë¶„ì„ ë©”ì¸ ë‹µë³€ ë‹¤ìŒì— í‘œì‹œ
        if reasoning_text:
            st.markdown("---")
            st.markdown("### ğŸ§  AIì˜ ì‚¬ê³  ê³¼ì •")
            with st.expander("ğŸ’­ Reasoning ê³¼ì • ë³´ê¸°", expanded=False):
                st.markdown(reasoning_text)

        # ëª¨ë“  ì°¸ì¡° ë¬¸ì„œ ë§í¬ë¥¼ ë§ˆì§€ë§‰ì— í‘œì‹œ
        if collected_data.get("sources_gathered"):
            st.markdown("---")
            st.markdown("### ğŸ“š ì°¸ì¡° ë¬¸ì„œ")

            # ì¤‘ë³µ ì œê±°ëœ ì†ŒìŠ¤ ëª©ë¡
            unique_sources = {}
            for source in collected_data["sources_gathered"]:
                url = source.get("value", "")
                if url and url not in unique_sources:
                    unique_sources[url] = source

            for i, (url, source) in enumerate(unique_sources.items(), 1):
                title = source.get("label", f"ë¬¸ì„œ {i}")
                st.markdown(f"{i}. [{title}]({url})")

    def _enhance_citations(self, text, collected_data):
        """í…ìŠ¤íŠ¸ì˜ ì¸ìš© ë²ˆí˜¸ë¥¼ ì‹¤ì œ URL ë§í¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not collected_data.get("sources_gathered"):
            return text

        # short_urlì„ í‚¤ë¡œ í•˜ëŠ” ì†ŒìŠ¤ ë§¤í•‘ ìƒì„±
        source_mapping = {}
        for source in collected_data["sources_gathered"]:
            short_url = source.get("short_url", "")
            if short_url:
                source_mapping[short_url] = source

        # [label](short_url) íŒ¨í„´ ì°¾ê¸°
        import re

        citation_pattern = r"\[([^\]]+)\]\(([^)]+)\)"

        def replace_citation(match):
            label = match.group(1)
            short_url = match.group(2)

            # short_urlë¡œ ì‹¤ì œ ì†ŒìŠ¤ ì°¾ê¸°
            if short_url in source_mapping:
                source = source_mapping[short_url]
                url = source.get("url", source.get("value", ""))

                if url:
                    # ì‹¤ì œ URLë¡œ ë§í¬ ë³€í™˜
                    return f"[[{label}]]({url})"

            # ë§¤ì¹­ë˜ëŠ” ì†ŒìŠ¤ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ìœ ì§€
            return match.group(0)

        # ëª¨ë“  ì¸ìš©ì„ ì‹¤ì œ ë§í¬ë¡œ ë³€í™˜
        enhanced_text = re.sub(citation_pattern, replace_citation, text)
        return enhanced_text

    def clean_answer_for_session(self, final_answer):
        """ì„¸ì…˜ ì €ì¥ìš©ìœ¼ë¡œ <think> íƒœê·¸ë¥¼ ì œê±°í•œ ê¹¨ë—í•œ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        clean_answer = re.sub(
            r"<think>.*?</think>", "", final_answer, flags=re.DOTALL
        ).strip()
        return clean_answer or final_answer

    def fallback_invoke(self, prompt, config):
        """ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ì¼ë°˜ invokeë¥¼ ì‹œë„í•©ë‹ˆë‹¤."""
        try:
            self.sidebar_manager.update_status(
                "ğŸ”„ í›„ì²˜ë¦¬ ì¤‘", "ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ê²°ê³¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."
            )

            result = self.graph.invoke(
                {"messages": [HumanMessage(content=prompt)]}, config
            )

            if "messages" in result and result["messages"]:
                for msg in reversed(result["messages"]):
                    if (
                        hasattr(msg, "content")
                        and msg.content
                        and msg.content != prompt
                    ):
                        return msg.content
            return None
        except Exception as invoke_error:
            st.error(f"ì¼ë°˜ invoke ì¤‘ ì˜¤ë¥˜: {str(invoke_error)}")
            return None
